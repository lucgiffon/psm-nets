import keras
import numpy as np
import keras.backend as K
import scipy.stats
from keras import initializers, activations

from skluc.utils.datautils import build_hadamard, dimensionality_constraints
import tensorflow as tf


def G_variable(shape, trainable=False):
    """
    Return a Gaussian Random matrix converted into Tensorflow Variable.

    :param shape: The shape of the matrix (number of fastfood stacks (v), dimension of the input space (d))
    :type shape: int or tuple of int (tuple size = 2)
    :return: K.variable object containing the matrix, The norm2 of each line (np.array of float)
    """
    assert type(shape) == int or (type(shape) == tuple and len(shape) == 2)
    G = np.random.normal(size=shape).astype(np.float32)
    G_norms = np.linalg.norm(G, ord=2, axis=1)
    if trainable:
        return K.variable(G, name="G"), G_norms
    else:
        return K.constant(G, name="G"), G_norms


def B_variable(shape, trainable=False):
    """
    Return a random matrix of -1 and 1 picked uniformly and converted into Tensorflow Variable.

    :param shape: The shape of the matrix (number of fastfood stacks (v), dimension of the input space (d))
    :type shape: int or tuple of int (tuple size = 2)
    :return: K.variable object containing the matrix
    """
    assert type(shape) == int or (type(shape) == tuple and len(shape) == 2)
    B = np.random.choice([-1, 1], size=shape, replace=True).astype(np.float32)
    if trainable:
        return K.variable(B, name="B")
    else:
        return K.constant(B, name="B")


def P_variable(d, nbr_stack):
    """
    Return a permutation matrix converted into Tensorflow Variable.

    :param d: The width of the matrix (dimension of the input space)
    :type d: int
    :param nbr_stack: The height of the matrix (nbr_stack x d is the dimension of the output space)
    :type nbr_stack: int
    :return: K.variable object containing the matrix
    """
    idx = np.hstack([(i * d) + np.random.permutation(d) for i in range(nbr_stack)])
    P = np.random.permutation(np.eye(N=nbr_stack * d))[idx].astype(np.float32)
    return K.constant(P, name="P")


def H_variable(d):
    """
    Return an Hadamard matrix converted into Tensorflow Variable.

    d must be a power of two.

    :param d: The size of the Hadamard matrix (dimension of the input space).
    :type d: int
    :return: K.variable object containing the diagonal and not trainable
    """
    H = build_hadamard(d).astype(np.float32)
    return K.constant(H, name="H")


def S_variable(shape, G_norms, trainable=False):
    """
    Return a scaling matrix of random values picked from a chi distribution.

    The values are re-scaled using the norm of the associated Gaussian random matrix G. The associated Gaussian
    vectors are the ones generated by the `G_variable` function.

    :param shape: The shape of the matrix (number of fastfood stacks (v), dimension of the input space (d))
    :type shape: int or tuple of int (tuple size = 2)
    :param G_norms: The norms of the associated Gaussian random matrices G.
    :type G_norms: np.array of floats
    :return: K.variable object containing the matrix.
    """
    S = np.multiply((1 / G_norms.reshape((-1, 1))), scipy.stats.chi.rvs(shape[1], size=shape).astype(np.float32))
    if trainable:
        return K.variable(S, name="S")
    else:
        return K.constant(S, name="S")


class FastFoodLayer(keras.layers.Layer):
    def __init__(self, nbr_stack, use_bias=True, activation=None, bias_initializer="zeros", sigma=1., cos_sin_act=False, trainable=True, **kwargs):
        super().__init__(**kwargs)
        self.__sigma = sigma
        self.__nbr_stack = nbr_stack
        self.__trainable = trainable
        self.__cos_sin_act = cos_sin_act

        self.use_bias = use_bias
        self.activation = activations.get(activation)
        self.bias_initializer = initializers.get(bias_initializer)

        self.__init_dim = None
        self.__final_dim = None

        super().__init__(**kwargs)

    def build(self, input_shape):
        with K.name_scope("fastfood" + "_sigma-" + str(self.__sigma)):
            self.__init_dim = np.prod([s for s in input_shape if s is not None])
            self.__final_dim = int(dimensionality_constraints(self.__init_dim))
            self.num_outputs = None

            G, G_norm = G_variable((self.__nbr_stack, self.__final_dim))
            self.__G = self.add_weight(
                name="G",
                shape=(self.__nbr_stack, self.__final_dim),
                initializer=lambda *args, **kwargs: G,
                trainable=self.__trainable
            )

            B = B_variable((self.__nbr_stack, self.__final_dim))
            self.__B = self.add_weight(
                name="B",
                shape=(self.__nbr_stack, self.__final_dim),
                initializer=lambda *args, **kwargs: B,
                trainable=self.__trainable
            )

            H = H_variable(self.__final_dim)
            self.__H = self.add_weight(
                name="H",
                shape=(self.__final_dim, self.__final_dim),
                initializer=lambda *args, **kwargs: H,
                trainable=False
            )

            P = P_variable(self.__final_dim, self.__nbr_stack)
            self.__P = self.add_weight(
                name="P",
                shape=(self.__final_dim * self.__nbr_stack, self.__final_dim * self.__nbr_stack),
                initializer=lambda *args, **kwargs: P,
                trainable=False
            )

            S = S_variable((self.__nbr_stack, self.__final_dim), G_norm)
            self.__S = self.add_weight(
                name="S",
                shape=(self.__final_dim * self.__nbr_stack, self.__final_dim * self.__nbr_stack),
                initializer=lambda *args, **kwargs: S,
                trainable=self.__trainable
            )

        self.num_outputs = self.__final_dim * self.__nbr_stack

        if self.use_bias:
            self.bias = self.add_weight(name="bias", shape=(self.num_outputs,), initializer=self.bias_initializer, trainable=True)


        super().build(input_shape)

    def call(self, input, **kwargs):
        padding = self.__final_dim - self.__init_dim
        conv_out2 = K.reshape(input, [-1, self.__init_dim])
        paddings = K.constant([[0, 0], [0, padding]], dtype=np.int32)
        conv_out2 = tf.pad(conv_out2, paddings, "CONSTANT")
        conv_out2 = K.reshape(conv_out2, (1, -1, 1, self.__final_dim))
        h_ff1 = conv_out2 * self.__B
        h_ff1 = K.reshape(h_ff1, (-1, self.__final_dim))
        h_ff2 = K.dot(h_ff1, self.__H)
        h_ff2 = K.reshape(h_ff2, (-1, self.__final_dim * self.__nbr_stack))
        h_ff3 = K.dot(h_ff2, self.__P)
        h_ff4 = K.reshape(h_ff3, (-1, self.__final_dim * self.__nbr_stack)) * K.reshape(self.__G, (-1, self.__final_dim * self.__nbr_stack))
        h_ff4 = K.reshape(h_ff4, (-1, self.__final_dim))
        h_ff5 = K.dot(h_ff4, self.__H)

        h_ff6 = (1 / (self.__sigma * np.sqrt(self.__final_dim))) * K.reshape(h_ff5, (-1, self.__final_dim * self.__nbr_stack)) * K.reshape(self.__S, (-1, self.__final_dim * self.__nbr_stack))
        if self.__cos_sin_act:
            h_ff7_1 = K.cos(h_ff6)
            h_ff7_2 = K.sin(h_ff6)
            h_ff7 = np.sqrt(float(1 / self.__final_dim)) * K.concatenate([h_ff7_1, h_ff7_2], axis=1)

        if self.use_bias:
            out = h_ff7 + self.bias
        else:
            out = h_ff7

        if self.activation is not None:
            out = self.activation(out)

        return out

    def compute_output_shape(self, input_shape):
        if self.__cos_sin_act:
            return (input_shape[0], 2 * self.__final_dim * self.__nbr_stack)
        else:
            return (input_shape[0], self.__final_dim * self.__nbr_stack)

    def get_config(self):
        base_config = super().get_config()
        base_config.update({
            "use_bias": self.use_bias,
            "nbr_stack": self.__nbr_stack,
            'trainable': self.__trainable,
            'activation': activations.serialize(self.activation),
            'bias_initializer': initializers.serialize(self.bias_initializer),
            'sigma': self.__sigma,
            "cos_sin_act": self.__cos_sin_act
        })
        return base_config