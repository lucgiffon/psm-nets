{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e26c15",
   "metadata": {},
   "source": [
    "# Tutorial: Layer weights as Products of Sparse Matrices (PSM)\n",
    "\n",
    "This notebook is a short tutorial on how to use the `pyfaust` and `tf.keras` libraries to compress a neural network\n",
    "into a PSM neural network.\n",
    "\n",
    "It is written in 4 parts:\n",
    "\n",
    "- Train a small network to achieve a toy classification task.\n",
    "- Use the pyfaust library to factorize the network weight matrices into product of sparse matrices\n",
    "- Show an implementation of a PSM layer that can handle weights expressed in product of sparse matrices\n",
    "- Build the PSM network, load the factorized weights and finetune\n",
    "\n",
    "### References\n",
    "\n",
    " - [PSM-nets paper](https://hal.archives-ouvertes.fr/hal-03151539): Luc Giffon, Stéphane Ayache, Hachem Kadri, Thierry Artières, Ronan Sicre. PSM-nets: Compressing Neural Networks with Product of Sparse Matrices. 2021.\n",
    " - [Faµst](https://faust.inria.fr/): Le Magoarou L. and Gribonval R., “Flexible multi-layer sparse approximations of matrices and applications”, Journal of Selected Topics in Signal Processing, 2016.\n",
    " - [Scikit-learn](https://scikit-learn.org/stable/index.html): Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    " - [Tensorflow](https://www.tensorflow.org/): Martín Abadi et al., TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.\n",
    " \n",
    " ### Requirements\n",
    " \n",
    " - tensorflow\n",
    " - scikit-learn\n",
    " - pyfaust\n",
    " \n",
    " To install the requirements, uncomment and execute the following line. Make sure you use a virtualenvironment to prevent eventual issues.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77cd852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow scikit-learn pyfaust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7555b",
   "metadata": {},
   "source": [
    "## Train a small network to achieve a toy classification task.\n",
    "\n",
    "We choose to train a simple, two layers, feed-forward neural network on the simple digit classification task provided by\n",
    "scikit-learn. It is a 10 classes classification task with input images of size (8x8x1). For our network, the input images\n",
    "are kept in vector form and the labels are one-hot encoded.\n",
    "\n",
    "### Load the classification dataset: digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40319583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8d46d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54170938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations shape: (64,); Labels shape (1, 10)\n",
      "Train data length: 1203; Test data length: 594\n"
     ]
    }
   ],
   "source": [
    "# prepare train and test datasets with one hot encoded labels\n",
    "X, y = load_digits(return_X_y=True)\n",
    "y = OneHotEncoder().fit_transform(y.reshape(-1, 1)).todense()\n",
    "print(f\"Observations shape: {X[0].shape}; Labels shape {y[0].shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(f\"Train data length: {len(X_train)}; Test data length: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c83378e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f53bf450690>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0].reshape(8, 8), cmap=plt.get_cmap(\"Greys\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb39d68",
   "metadata": {},
   "source": [
    "### Build & fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47d2b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 840us/step - loss: 0.0878 - accuracy: 0.9747\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Base model accuracy: 97.47%.\n"
     ]
    }
   ],
   "source": [
    "n_features = X_train.shape[1]\n",
    "# prepare the two layer neural network\n",
    "model = Sequential()\n",
    "# we use a purposedly oversized hidden dense layer for the proof of concept.\n",
    "model.add(Dense(1000, input_shape=(n_features,), activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "base_accuracy = model.evaluate(X_test, y_test)[1] * 100\n",
    "print(f\"Base model accuracy: {base_accuracy:.2f}%.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685da4d",
   "metadata": {},
   "source": [
    "## Use the pyfaust library to factorize the network weight matrices into product of sparse matrices\n",
    "\n",
    "After the network have been trained, we want to extract the weight matrices of the dense layers and compress them\n",
    "using the `pyfaust` library. The `pyfaust` library provides a simple interface for the PALM4MSA algorithm and its\n",
    "hierarchical variant. Here we only use the simple PALM4MSA algorithm. The constraint we use for each sparse factor is\n",
    "called `splincol`. It takes the target sparse factor as input and one parameter which we call the \"sparsity level\".\n",
    "It ensures that all the output sparse factors have the right shape and at least \"sparsity level\" coefficients on each\n",
    "row and each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8d7301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfaust.fact import palm4msa\n",
    "from pyfaust.factparams import ParamsPalm4MSA, StoppingCriterion\n",
    "from pyfaust.proj import splincol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7deb0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dct_layer_faust_obj_biases = dict()\n",
    "sparsity_level = 5\n",
    "\n",
    "for layer in model.layers:\n",
    "    if not isinstance(layer, Dense):\n",
    "        # we only want to compress dense layers here\n",
    "        print(f\"Not a dense layer: {layer.__class__.__name__}\")\n",
    "    else:\n",
    "        weights, biases = layer.get_weights()\n",
    "        # the \"inner\" dimension of each sparse factor is the min between the two dimensions of the input weight matrix\n",
    "        A = min(weights.shape)\n",
    "\n",
    "        # the number of projectors specifies the number of output sparse factors. Here: 2.\n",
    "        lst_projs = [\n",
    "            splincol((weights.shape[0], A),sparsity_level),\n",
    "            splincol((A, weights.shape[1]), sparsity_level)\n",
    "        ]\n",
    "\n",
    "        # 200 iteration is arbitrary and have been shown \n",
    "        # to give sufficiently good results\n",
    "        stop_crit = StoppingCriterion(num_its=200) \n",
    "\n",
    "        # the palm4msa function will factorize the weights variable into factors whose constrained are \n",
    "        # specified inside the ParamsPalm4MSA object\n",
    "        param = ParamsPalm4MSA(lst_projs, stop_crit)\n",
    "        # ret_lambda allows to return the disambiguating scaling factor\n",
    "        faust_weights = palm4msa(weights.astype(float), param, ret_lambda=True)\n",
    "        \n",
    "        dct_layer_faust_obj_biases[layer.name] = (*faust_weights, biases)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152e002",
   "metadata": {},
   "source": [
    "### Show an implementation of a PSM layer that can handle weights expressed in product\n",
    "\n",
    "To be clear: the proposed implementation isn't purposed to be used in production for model compression. It can't.\n",
    "The implementation we propose for \"PSM\"-Dense layers actually contains even more parameters than a classical dense layer.\n",
    "However, it mimics the computation of sparse factors and it allows us to see that PSM layers are capable to learn to\n",
    "achieve a given learning task.\n",
    "\n",
    "In earlier version of tensorflow, it was possible to train actual sparse tensors. However, the implementation with the\n",
    "SparseTensor class was much slower than this one. It makes sense since `tensorflow` and the GPU modern libraries have\n",
    "been designed with dense matrices multiplications in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc47f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07af6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSMDense(Layer):\n",
    "    \"\"\"\n",
    "    Layer which implements a sparse factorization with fixed sparsity pattern for all factors.\n",
    "    The gradient will only be computed for non-zero entries.\n",
    "    `SparseFactorisationDense` implements the operation:\n",
    "    `output = activation(dot(input, prod([kernel[i] * sparsity_pattern[i] for i in range(nb_factor)]) + bias)`\n",
    "    where `activation` is the element-wise activation function\n",
    "    passed as the `activation` argument.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, sparsity_patterns,\n",
    "                 activation=None,\n",
    "                 **kwargs):\n",
    "\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "\n",
    "        super(PSMDense, self).__init__(**kwargs)\n",
    "\n",
    "        # note that the sparsity patterns for the sparse factors are already fixed at init.\n",
    "        self.sparsity_patterns = sparsity_patterns\n",
    "        self.nb_factor = len(sparsity_patterns)\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "\n",
    "        self.build(input_shape=(None, sparsity_patterns[-1].shape[1]))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "\n",
    "        self.kernels = []  # this will contain the actual weights to be learned\n",
    "        self.sparsity_masks = []  # this will contain the constant mask corresponding to each weight matrix\n",
    "        for i in range(self.nb_factor):\n",
    "\n",
    "            kernel = self.add_weight(shape=self.sparsity_patterns[i].shape,\n",
    "                                     name='kernel_{}'.format(i))\n",
    "            sparsity_mask = tf.constant(self.sparsity_patterns[i], dtype=tf.float32, name=\"sparsity_mask_{}\".format(i))\n",
    "\n",
    "            self.kernels.append(kernel)\n",
    "            self.sparsity_masks.append(sparsity_mask)\n",
    "\n",
    "        self.bias = self.add_weight(shape=(self.units,),\n",
    "                                    name='bias')\n",
    "\n",
    "        super(PSMDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = inputs\n",
    "\n",
    "        for i in range(self.nb_factor):\n",
    "            # the pairwise multiplication with the weight matrix make the gradient zero \n",
    "            # where the mask has value 0.\n",
    "            output = tf.matmul(output, self.kernels[i] * self.sparsity_masks[i])\n",
    "\n",
    "        output = tf.nn.bias_add(output, self.bias)\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.units\n",
    "\n",
    "    def set_weights(self, lst_weights):\n",
    "        bias = lst_weights[-1]\n",
    "        # we remove all the weights outside the 1 of the sparsity masks.\n",
    "        # This doesn't change anything on the actual computation but helps keep things clean and \n",
    "        # allows for easier count of non-zero weights.\n",
    "        super().set_weights([lst_weights[i] * self.sparsity_patterns[i] for i in range(self.nb_factor)] + [bias])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5079a",
   "metadata": {},
   "source": [
    "### Build the PSM network, load the factorized weights and finetune\n",
    "\n",
    "Every dense layer of the initial model is replaced with a PSM version of it. The weights and sparsity masks of the\n",
    "PSM layers are initialized with the factorized weight matrices from the pretrained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26408e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 811us/step - loss: 84.0327 - accuracy: 0.9495\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "New model accuracy: 94.95\n"
     ]
    }
   ],
   "source": [
    "new_model = Sequential()\n",
    "\n",
    "for layer in model.layers:\n",
    "    if not isinstance(layer, Dense):        \n",
    "        print(f\"Not a dense layer: {layer.__class__.__name__}\")\n",
    "        new_model.add(layer.__class__(**layer.get_config()))\n",
    "    else:\n",
    "        (faust_weights, scaling, biases) = dct_layer_faust_obj_biases[layer.name]\n",
    "        # the sparse factors are cast into np.ndarray so now they are not \"sparse\" anymore, memory-wise.\n",
    "        lst_array_faust_weights = [faust_weights.factors(i).toarray() if not isinstance(faust_weights.factors(i), np.ndarray) else faust_weights.factors(i) for i in range(faust_weights.numfactors())]\n",
    "        # the first sparse factor is rescaled for the full sparse factorization to really \n",
    "        # approximate the initial weight matrix\n",
    "        lst_array_faust_weights[0] = scaling * lst_array_faust_weights[0]\n",
    "        lst_sparsity_pattern = [elm.astype(bool).astype(float) for elm in lst_array_faust_weights]\n",
    "        new_layer = PSMDense(layer.units,\n",
    "                                             sparsity_patterns=lst_sparsity_pattern,\n",
    "                                             activation=layer.activation)\n",
    "        new_model.add(new_layer)\n",
    "        new_layer.set_weights( lst_array_faust_weights + [biases])\n",
    "new_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "new_model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "accuracy_new_model = new_model.evaluate(X_test, y_test)[1] * 100\n",
    "print(f\"New model accuracy: {accuracy_new_model:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec707fd8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_nnz_weights(tfk_model):\n",
    "    \"\"\"\n",
    "    Counts the number of non-zero weights in a model.\n",
    "\n",
    "    :param tfk_model: a tf.keras model.\n",
    "    :return: int\n",
    "    \"\"\"\n",
    "    total_weight_count = 0\n",
    "    for layer in tfk_model.layers:\n",
    "        lst_weights = layer.get_weights()\n",
    "        for weights in lst_weights:\n",
    "            # casting a matrix to bool make its coefficients 1 where they were different than 0 and 0 otherwise.\n",
    "            # summing the \"bool-ified\" matrix is equivalent to counting the non-zero coffecients\n",
    "            total_weight_count += np.sum(weights.astype(bool))\n",
    "    return total_weight_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ae6bf63",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights in the base model: 74986\n",
      "Number of weights in the compressed model: 11443\n",
      "The PSM network has 6.6 times less non-zero learnable parameters than the base network.\n"
     ]
    }
   ],
   "source": [
    "nb_weights_base = count_nnz_weights(model)\n",
    "nb_weights_compressed = count_nnz_weights(new_model)\n",
    "ratio = nb_weights_base / nb_weights_compressed\n",
    "print(f\"Number of weights in the base model: {nb_weights_base}\")\n",
    "print(f\"Number of weights in the compressed model: {nb_weights_compressed}\")\n",
    "print(f\"The PSM network has {ratio:.1f} times less non-zero learnable parameters than the base network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "857e6011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}